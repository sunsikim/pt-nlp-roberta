# pt-nlp-roberta
How Pytorch implementation of Roberta pretraining logic might look like in production
